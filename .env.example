# TLC LLM Playground - Environment Configuration Template
# Copy this file to .env and customize for your environment

# LM Studio URL - where your local LLM server is running
# IMPORTANT: Include /v1 suffix for OpenAI-compatible client
LM_STUDIO_URL=http://localhost:1234/v1

# Queue Server Configuration
QUEUE_HOST=0.0.0.0
QUEUE_PORT=8000
