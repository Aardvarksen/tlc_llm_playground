Ratings - ‚úÖüü°‚ùå
Answer these as if explaining to a colleague who's about to work on the codebase. Don't look at the code while answering - write from memory/understanding.

### Q1
From the Streamlit side, instead of sending a request directly to the LLM's endpoint it will instead send the request with most of the same info to the queue server instead, where it joins the end of the Queue. The queue server's response will give the initial size of the queue which Streamlit saves as a variable value, and then continues to send total size of the queue to give the user an indication of how busy it is. Once that request reaches the front of the queue, the worker will pick it up and pass the info to the LLM endpoint, and then continue to stream the responses to Streamlit until it completes.

### Q2
The main problem is that LM Studio has "just in time" model loading when it receives a request, and will unload. Since this playground is explicitly to allow the TLC department to explore and see results many different LLM options, the issue arises when a new model is selected while an initial model is still generating. The initial model unloads, causing an error for the user.

### Q3
The data structure lives in memory, as an asyncio.Queue class type. Currently, it is entirely wiped when the server restarts

### Q4
‚ùå We used async because Claude suggested it and had a very reasonable-sounding explanation at the time lol

### Q5
‚ùå This is something that is part of the asyncio library. I believe it is the loop that assigns threads to tasks (?), and our worker is the only task, but I'm not sure if the 'event loop' is what our worker is doing, or if it is some actual thing that the asyncio library is doing when in use.

### Q6
üü° The system handles the 8 requests at once because most of the activity is 'await' or maybe 'get status' requests (concurrently). Beyond that, they are just sitting in the queue until the worker picks them up (sequentially)

### Q7
(this may be cheating, but I was just actually telling a colleague about the /docs endpoint and have it open...)
The request is initially sent with a POST request to /queue/add, and the data sent must follow the QueueRequest schema. From there is sits in the asyncio.Queue() object until picked up by the worker. Then when the worker picks it up and sends the request to the LLM endpoint, it receives the stream of chunks, stores them in something I don't remember, and then sends the stream onward to the Streamlit client.

### Q8
This is a unique identifier to make sure that clients get their own actual responses back instead of responses from some other request.

### Q9
There is a request/{request_id} endpoint that shows the status of a given request. The queue server creates a UUID, and after the post request is received, there is an initial response sending the UUID back to the Streamlit client. 

### Q10
‚ùå

### Q11
‚ùå something something {"stream": True}, something something 'generator'

### Q12
Having streaming is a user experience consideration. The negative tradeoff is added complexity, but for larger models that might only be getting single-digit tokens per second, this gives the user something to read while the rest of it finishes generating. It effectively reduces the "time to first token" metric from the end user's point of view

### Q13
The streamlit app connects via the requests library to the queue server's URL. There is a POST request to /queue/add, continual GET /request/{request_id} status checks until the worker picks up the request, and then the GET /stream/{request_id} request to get the streaming tokens from the queue server.

### Q14
The session state stores things like the messages list, the model selected and other inference parameters. If we didn't have session_state to reference, then we'd have to store and retrieve those things separately.

### Q15
This data comes from a qsize() function that is part of asyncio. It comes from the part of the response from the GET /request/{request_id} call

### Q16


