# LiteLLM Proxy Configuration
# ============================
# Translates Anthropic API requests (from Claude Code) to OpenAI format
# and routes them through the queue server to LM Studio.
#
# Start with: litellm --config litellm_config.yaml --host 0.0.0.0 --port 4000

model_list:
  # Primary coding model - use this name in Claude Code
  # Maps to qwen3-32b-128k loaded in LM Studio
  - model_name: qwen3-coder
    litellm_params:
      model: openai/qwen3-coder-30b-a3b      # Must match LM Studio's model ID exactly
      api_base: http://localhost:8001/v1  # Queue server (NOT direct to LM Studio)
      api_key: "not-needed"
      timeout: 600                        # 10 min for long generations
      stream_timeout: 300                 # 5 min wait for first chunk (queue time)

litellm_settings:
  drop_params: true           # Ignore params the backend doesn't support
  set_verbose: false          # Set to true for debugging

general_settings:
  master_key: null            # No auth for now (internal network)
