"""
Queue Server for TLC LLM Playground
====================================

This server manages a queue of LLM requests and processes them sequentially,
streaming responses back to clients in real-time.

This is built incrementally to understand each piece:
1. Basic FastAPI setup ‚úÖ
2. Request queue data structure ‚úÖ
3. Endpoints for adding/checking requests ‚úÖ
4. Background worker for processing ‚úÖ
5. Streaming responses via Server-Sent Events ‚úÖ
6. Integration with LM Studio ‚úÖ
7. Ready for testing!
"""

import asyncio
import uuid
from datetime import datetime
from typing import Optional

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from openai import AsyncOpenAI

# Import our centralized configuration
import config

# Create the FastAPI application instance
# This is the main object that handles all HTTP requests
app = FastAPI(
    title="LLM Queue Server",
    description="Manages queued LLM requests with streaming responses",
    version="0.1.0"
)


# ============================================================================
# STEP 2: Data Models (Pydantic Schemas)
# ============================================================================
# These define the structure of data coming in/out of the API
# Pydantic handles validation and auto-generates API documentation

class QueueRequest(BaseModel):
    """
    Request from a client to add an LLM completion to the queue.

    Follows OpenAI API format - the client is responsible for constructing
    the full messages array (including system prompt, context, history).

    The queue server is a "dumb pass-through" - it just queues and forwards.
    """
    client_id: str  # Which app sent this? "streamlit_demo_1", "jupyter_lab", etc.
    messages: list[dict]  # OpenAI format: [{"role": "system", "content": "..."}, ...]
    model: str  # Which LLM model to use

    # LLM generation parameters (matching OpenAI API)
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stream: bool = True  # We always stream responses

    # Optional: for future per-user auth within a client app
    user: Optional[str] = None


class QueueResponse(BaseModel):
    """
    Response sent back when a request is successfully queued.
    """
    status: str  # "queued"
    request_id: str  # UUID generated by server
    queue_position: int  # How many requests ahead of this one


class RequestStatus(BaseModel):
    """
    Tracks the current status of a request.
    Stored in the status_tracker dict.
    """
    status: str  # "queued" | "processing" | "complete" | "error"
    client_id: str
    created_at: str
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    result: Optional[str] = None  # Final generated text (if not streaming)
    error: Optional[str] = None  # Error message if something went wrong


# ============================================================================
# STEP 2: Queue Data Structures
# ============================================================================
# These hold our request queue and status tracking in memory

# The main request queue - asyncio.Queue is async-safe and won't block the event loop
# Items in the queue are dicts with: {request_id, client_id, request_data}
request_queue: asyncio.Queue = asyncio.Queue()

# Status tracking - maps request_id -> RequestStatus
# This lets clients check on their request without polling the queue itself
status_tracker: dict[str, RequestStatus] = {}

# Queue statistics for monitoring
queue_stats = {
    "total_received": 0,  # How many requests ever submitted
    "total_processed": 0,  # How many completed successfully
    "total_errors": 0,  # How many failed
    "current_request_id": None,  # Which request is currently being processed
}

# Worker control flag - set to False to stop the worker gracefully
worker_running = True

# OpenAI client for LM Studio
# AsyncOpenAI is the async version of the OpenAI client
llm_client = AsyncOpenAI(
    base_url=config.LM_STUDIO_BASE_URL,
    api_key=config.LM_STUDIO_API_KEY  # LM Studio doesn't check this, but client requires it
)

# Stream chunks storage
# For each request being processed, we create a queue to hold streaming chunks
# The worker pushes chunks here, the streaming endpoint pulls from here
# Maps request_id -> asyncio.Queue[str]
stream_chunks: dict[str, asyncio.Queue] = {}


# ============================================================================
# STEP 4: Background Worker
# ============================================================================
# This is the "engine" that actually processes queued requests.
# It runs continuously in the background, pulling requests and processing them.

async def queue_worker():
    """
    Background worker that processes requests from the queue.

    This runs as a long-lived async task that:
    1. Waits for requests in the queue (blocks if empty, doesn't busy-loop!)
    2. Pulls a request when available
    3. Updates status to "processing"
    4. Processes the request (calls LM Studio - we'll add this in step 6)
    5. Updates status to "complete" or "error"
    6. Updates statistics
    7. Loops back to step 1

    This is an INFINITE LOOP that only stops when worker_running is set to False.

    Key concepts:
    - `await request_queue.get()` blocks until an item is available
    - asyncio.sleep() allows other tasks to run (doesn't block event loop)
    - try/except ensures one failed request doesn't crash the worker
    """
    print("üîß Queue worker started - waiting for requests...")

    while worker_running:
        try:
            # Wait for a request from the queue
            # This BLOCKS (yields control) until a request is available
            # The `timeout=1` means we check every second if we should shutdown
            try:
                queue_item = await asyncio.wait_for(request_queue.get(), timeout=1.0)
            except asyncio.TimeoutError:
                # No item in queue, loop and try again
                # This lets us check worker_running flag regularly
                continue

            # Extract data from queue item
            request_id = queue_item["request_id"]
            client_id = queue_item["client_id"]
            request_data = queue_item["request_data"]

            print(f"üìù Processing request {request_id} from {client_id}")

            # Update status to "processing"
            status_tracker[request_id].status = "processing"
            status_tracker[request_id].started_at = datetime.now().isoformat()
            queue_stats["current_request_id"] = request_id

            # ============================================================
            # ACTUAL PROCESSING HAPPENS HERE
            # ============================================================
            # Call LM Studio with streaming and forward chunks to the stream endpoint

            try:
                # Create a queue for this request's streaming chunks
                # The streaming endpoint will read from this queue
                stream_chunks[request_id] = asyncio.Queue()

                # Extract parameters from request
                messages = request_data.get("messages", [])
                model = request_data.get("model")
                temperature = request_data.get("temperature", 0.7)
                max_tokens = request_data.get("max_tokens")
                top_p = request_data.get("top_p", 1.0)
                frequency_penalty = request_data.get("frequency_penalty", 0.0)
                presence_penalty = request_data.get("presence_penalty", 0.0)

                print(f"ü§ñ Calling LM Studio with model: {model}")

                # Call LM Studio with streaming
                # This returns an async generator that yields chunks as they arrive
                stream = await llm_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    frequency_penalty=frequency_penalty,
                    presence_penalty=presence_penalty,
                    stream=True  # CRITICAL: enables streaming
                )

                # Collect the full response for storage
                full_response = ""

                # Process the stream
                async for chunk in stream:
                    # Each chunk from OpenAI has this structure:
                    # chunk.choices[0].delta.content = "the text"
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content

                        # Push this chunk to the streaming queue
                        # The streaming endpoint will pull from here and send to client
                        await stream_chunks[request_id].put(content)

                # Send a "done" signal to the streaming endpoint
                await stream_chunks[request_id].put(None)  # None = stream finished

                # Update status to "complete"
                status_tracker[request_id].status = "complete"
                status_tracker[request_id].completed_at = datetime.now().isoformat()
                status_tracker[request_id].result = full_response

                # Update statistics
                queue_stats["total_processed"] += 1
                queue_stats["current_request_id"] = None

                print(f"‚úì Request {request_id} completed successfully ({len(full_response)} chars)")

            except Exception as processing_error:
                # If processing fails, mark as error
                print(f"‚úó Error processing request {request_id}: {processing_error}")

                # If we created a stream queue, send error signal and clean up
                if request_id in stream_chunks:
                    try:
                        await stream_chunks[request_id].put({"error": str(processing_error)})
                    except:
                        pass  # Queue might already be gone

                status_tracker[request_id].status = "error"
                status_tracker[request_id].completed_at = datetime.now().isoformat()
                status_tracker[request_id].error = str(processing_error)

                queue_stats["total_errors"] += 1
                queue_stats["current_request_id"] = None

        except Exception as e:
            # Catch-all for any unexpected errors in the worker loop
            # This prevents the worker from crashing
            print(f"‚úó Worker error: {e}")
            await asyncio.sleep(1)  # Brief pause before retrying

    print("üõë Queue worker stopped")


# ============================================================================
# FastAPI Lifecycle Events
# ============================================================================
# These hooks let us start/stop the background worker when the server starts/stops

@app.on_event("startup")
async def startup_event():
    """
    Runs when the FastAPI server starts up.
    We use this to launch the background worker task.
    """
    print("üöÄ Server starting up...")

    # Create the worker task and let it run in the background
    # asyncio.create_task() schedules it to run but doesn't wait for it
    asyncio.create_task(queue_worker())

    print("‚úì Background worker launched")


@app.on_event("shutdown")
async def shutdown_event():
    """
    Runs when the FastAPI server is shutting down.
    We use this to stop the background worker gracefully.
    """
    global worker_running

    print("üõë Server shutting down...")
    print("‚è≥ Stopping background worker...")

    # Signal the worker to stop
    worker_running = False

    # Give it a moment to finish current request
    await asyncio.sleep(2)

    print("‚úì Shutdown complete")


# ============================================================================
# STEP 3: Queue Management Endpoints
# ============================================================================

@app.post("/queue/add", response_model=QueueResponse)
async def add_to_queue(request: QueueRequest):
    """
    Add a new LLM request to the queue.

    This is the main entry point for clients. When a Streamlit app (or any client)
    wants to generate text, it POSTs here with the full OpenAI-format request.

    Flow:
    1. FastAPI validates the request against QueueRequest schema (automatic!)
    2. We generate a unique request_id (UUID)
    3. We create a status tracker entry
    4. We add the request to the queue
    5. We return the request_id and queue position

    Args:
        request: QueueRequest - validated by Pydantic

    Returns:
        QueueResponse with request_id and queue position
    """
    # Generate a unique request ID (UUID4 = random, practically guaranteed unique)
    request_id = str(uuid.uuid4())

    # Create status tracking entry
    # This lets clients poll for status even after request leaves queue
    status_tracker[request_id] = RequestStatus(
        status="queued",
        client_id=request.client_id,
        created_at=datetime.now().isoformat(),
    )

    # Package the request for the queue
    # The queue holds dicts with metadata + the original request data
    queue_item = {
        "request_id": request_id,
        "client_id": request.client_id,
        "request_data": request.dict(),  # Convert Pydantic model to dict
    }

    # Add to the queue (async operation - won't block event loop)
    await request_queue.put(queue_item)

    # Update statistics
    queue_stats["total_received"] += 1

    # Calculate current queue position (how many ahead of this request?)
    # qsize() gives current queue size, so position is qsize
    # (The request we just added is at the back)
    queue_position = request_queue.qsize()

    # Return response to client
    return QueueResponse(
        status="queued",
        request_id=request_id,
        queue_position=queue_position
    )


@app.get("/queue/status")
async def get_queue_status():
    """
    Get overall queue status and statistics.

    Useful for monitoring and debugging. Shows:
    - How many items are waiting
    - Which request is currently processing
    - Total requests received/processed/errors

    Returns:
        dict: Queue status and statistics
    """
    return {
        "queue_size": request_queue.qsize(),
        "stats": queue_stats,
        "tracked_requests": len(status_tracker)
    }


@app.get("/request/{request_id}")
async def get_request_status(request_id: str):
    """
    Get the status of a specific request.

    Clients can poll this to see if their request is queued/processing/complete.

    Args:
        request_id: The UUID returned from /queue/add

    Returns:
        RequestStatus if found, error if not
    """
    if request_id not in status_tracker:
        return {"error": "Request ID not found"}

    return status_tracker[request_id]


# ============================================================================
# STEP 5 & 6: Streaming Endpoint (Server-Sent Events)
# ============================================================================

@app.get("/stream/{request_id}")
async def stream_response(request_id: str):
    """
    Stream the LLM response for a request in real-time using Server-Sent Events (SSE).

    This endpoint:
    1. Waits for the request to start processing (if it's still queued)
    2. Opens a long-lived connection
    3. Forwards chunks from the worker as they arrive
    4. Closes when the response is complete

    The client should:
    1. POST to /queue/add to get a request_id
    2. Open a connection to /stream/{request_id}
    3. Read chunks as they arrive (SSE format)
    4. Close when it receives the "done" message

    SSE Format:
        data: {"chunk": "Hello"}

        data: {"chunk": " world"}

        data: {"done": true}

    Args:
        request_id: The UUID from /queue/add

    Returns:
        StreamingResponse with SSE-formatted chunks
    """
    # Check if request exists
    if request_id not in status_tracker:
        return {"error": "Request ID not found"}

    async def event_generator():
        """
        Async generator that yields SSE-formatted chunks.

        This is the actual streaming logic:
        - Waits for the request to be picked up by worker
        - Reads chunks from the stream_chunks queue
        - Formats them as SSE events
        - Yields them to the client
        """
        import json

        # Wait for the request to start processing
        # While it's queued, send position updates
        while status_tracker[request_id].status == "queued":
            queue_pos = request_queue.qsize()
            yield f"data: {json.dumps({'position': queue_pos})}\n\n"
            await asyncio.sleep(0.5)  # Check every 0.5s

        # If request errored out before processing started, send error and exit
        if status_tracker[request_id].status == "error":
            yield f"data: {json.dumps({'error': status_tracker[request_id].error})}\n\n"
            return

        # Wait for worker to create the stream queue
        # (There's a tiny window where status is "processing" but queue not created yet)
        while request_id not in stream_chunks:
            await asyncio.sleep(0.1)

        # Now stream chunks as they arrive!
        chunk_queue = stream_chunks[request_id]

        while True:
            # Get next chunk from the queue
            # This blocks until a chunk is available
            chunk = await chunk_queue.get()

            if chunk is None:
                # None means stream is done
                yield f"data: {json.dumps({'done': True})}\n\n"
                break

            elif isinstance(chunk, dict) and "error" in chunk:
                # Error during processing
                yield f"data: {json.dumps({'error': chunk['error']})}\n\n"
                break

            else:
                # Regular chunk - send it!
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"

        # Clean up - remove the stream queue
        if request_id in stream_chunks:
            del stream_chunks[request_id]

    # Return a StreamingResponse with SSE media type
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# ============================================================================
# STEP 1: Health Check Endpoint
# ============================================================================

@app.get("/")
def root():
    """
    Health check endpoint - confirms the server is running.

    Returns:
        dict: Simple status message
    """
    return {
        "status": "running",
        "message": "LLM Queue Server is operational",
        "version": "0.1.0"
    }


@app.get("/health")
def health_check():
    """
    Detailed health check - useful for monitoring.

    Returns:
        dict: Server health information including queue stats
    """
    return {
        "status": "healthy",
        "lm_studio_url": config.LM_STUDIO_BASE_URL,
        "queue_server_port": config.QUEUE_SERVER_PORT,
        "queue_size": request_queue.qsize(),
        "stats": queue_stats
    }


# ============================================================================
# How to run this server:
# ============================================================================
# From your terminal with venv activated:
#
#   Local access only:
#     uvicorn queue_server:app --reload --host 127.0.0.1 --port 8000
#
#   Network access (accessible from other machines):
#     uvicorn queue_server:app --reload --host 0.0.0.0 --port 8000
#
# Then visit:
#   http://localhost:8000           (health check)
#   http://localhost:8000/docs      (auto-generated API documentation)
#   http://localhost:8000/health    (detailed health info)
#
# The --reload flag makes the server restart when you save changes to this file
# --host 0.0.0.0 means "listen on all network interfaces" (accessible via VPN/LAN)
# --host 127.0.0.1 means "localhost only" (more secure, only same machine)
# ============================================================================
